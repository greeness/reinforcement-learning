{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reinforcement-learning/DP/Policy Evaluation.ipynb\n",
    "Start jupyter from inside ~/github/reinforcement-learning \n",
    "greeness-macbookpro:reinforcement-learning greeness$ jupyter notebook\n",
    "so that we can import lib.envs.gridworld as below.\n",
    "\n",
    "Copied from gridworld.py\n",
    "\"\"\"\n",
    "Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
    "You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "state at the top left or the bottom right corner.\n",
    "For example, a 4x4 grid looks as follows:\n",
    "T  o  o  o\n",
    "o  x  o  o\n",
    "o  o  o  o\n",
    "o  o  o  T\n",
    "x is your position and T are the two terminal states.\n",
    "You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
    "Actions going off the edge leave you in your current state.\n",
    "You receive a reward of -1 at each step until you reach a terminal state.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', '__class__', '__del__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_close', '_closed', '_env_closer_id', '_owns_render', '_render', '_reset', '_seed', '_step', 'action_space', 'close', 'configure', 'isd', 'lastaction', 'metadata', 'monitor', 'nA', 'nS', 'np_random', 'observation_space', 'render', 'reset', 'reward_range', 's', 'seed', 'shape', 'step', 'unwrapped']\n"
     ]
    }
   ],
   "source": [
    "print dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-inf, inf)\n"
     ]
    }
   ],
   "source": [
    "print env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 16 11 None\n",
      "{'render.modes': ['human', 'ansi']}\n"
     ]
    }
   ],
   "source": [
    "print env.nA, env.nS, env.s, env.lastaction\n",
    "print env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)], 2: [(1.0, 0, 0.0, True)], 3: [(1.0, 0, 0.0, True)]}\n",
      "{0: [(1.0, 15, 0.0, True)], 1: [(1.0, 15, 0.0, True)], 2: [(1.0, 15, 0.0, True)], 3: [(1.0, 15, 0.0, True)]}\n"
     ]
    }
   ],
   "source": [
    "# state 0 and state 15 are the two terminal states\n",
    "# Once you are at the two states (P[0] or P[15]), you transfer to the same state with probability of 1.0 and\n",
    "# reward 0 and done = true no mater what action you take (0, 1, 2, or 3).\n",
    "print env.P[0]\n",
    "print env.P[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(1.0, 8, -1.0, False)], 1: [(1.0, 13, -1.0, False)], 2: [(1.0, 12, -1.0, False)], 3: [(1.0, 12, -1.0, False)]}\n"
     ]
    }
   ],
   "source": [
    "# in other states, e.g. state 12 (fourth row, first column)\n",
    "# Actions going off the edge (action 2-DOWN or 3-LEFT) leave you in your current state \n",
    "# Actions in the valid direction moves you to the neighboring cell. (12->8 by action 0-UP and 12->13 by action 1-RIGHT)\n",
    "print env.P[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a (prob, next_state, reward, done) tuple.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    nIterations = 0\n",
    "    while True:\n",
    "        # TODO: Implement!\n",
    "        Delta = 0\n",
    "        for s in range(env.nS):\n",
    "            oldv = V[s]\n",
    "            newv = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    newv += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            Delta = max(Delta, abs(oldv - newv))\n",
    "            V[s] = newv\n",
    "        nIterations += 1\n",
    "        if nIterations % 10 == 0: \n",
    "            print Delta\n",
    "        if Delta < theta:                \n",
    "          break\n",
    "    \n",
    "    print nIterations\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 2, -1.0, False)]\n"
     ]
    }
   ],
   "source": [
    "print env.P[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8924059503\n",
      "0.372594031843\n",
      "0.155274167545\n",
      "0.0647083950673\n",
      "0.0269663422805\n",
      "0.0112378558487\n",
      "0.00468322335909\n",
      "0.00195166954678\n",
      "0.000813331700794\n",
      "0.000338944908272\n",
      "0.000141250673906\n",
      "5.88642944344e-05\n",
      "2.45308929401e-05\n",
      "1.02229155097e-05\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.         -13.99993529 -19.99990698 -21.99989761 -13.99993529\n",
      " -17.9999206  -19.99991379 -19.99991477 -19.99990698 -19.99991379\n",
      " -17.99992725 -13.99994569 -21.99989761 -19.99991477 -13.99994569   0.        ]\n"
     ]
    }
   ],
   "source": [
    "print v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
